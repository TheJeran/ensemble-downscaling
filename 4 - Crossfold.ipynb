{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e65c58-9bb9-4b9f-b0ae-13bbb3b6a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests as tfdf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def ubrmse(ground,pred):\n",
    "    bias = np.mean(ground-pred)\n",
    "    rmse = np.sqrt(np.mean((ground-pred)**2))\n",
    "    ubrmse = np.sqrt(rmse**2-bias**2)\n",
    "    return round(ubrmse,4)\n",
    "\n",
    "def nll(y_true, y_pred):\n",
    "    return -y_pred.log_prob(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab25b5d-6559-4a38-9f2f-8fef2d2fcd4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46165b-910c-4a0c-ad8f-d9cbed645b7e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3955965-4b7a-467f-a2c7-2f5303fed414",
   "metadata": {},
   "source": [
    "For training the RF ensemble, please use 2 - TFDF_Train.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80df6df-5d08-453a-bc70-f8b1e3c82d4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Non-RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f0f9b6-1345-48b1-ac4f-5a1cca50d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_pickle('dataframes/all_df_final.pkl')\n",
    "all_df = normalize_df(all_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dcd4e6-81f7-4c77-b39b-0752ddd00542",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c74dc-c8eb-4125-a2eb-0ba5d28ede6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prior(kernel_size,bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    prior_model = Sequential([\n",
    "        tfpl.DistributionLambda(\n",
    "            lambda t: tfd.MultivariateNormalDiag(loc=tf.zeros(n),scale_diag=tf.ones(n))\n",
    "        )\n",
    "    ])\n",
    "    return prior_model\n",
    "\n",
    "def posterior(kernel_size,bias_size, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    posterior_model = Sequential([\n",
    "        tfpl.VariableLayer(tfpl.MultivariateNormalTriL.params_size(n),dtype=dtype),\n",
    "        tfpl.MultivariateNormalTriL(n)\n",
    "    ])\n",
    "    return posterior_model\n",
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return tf.keras.Sequential([\n",
    "      tfp.layers.VariableLayer(n, dtype=dtype,name='prior_var'),\n",
    "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "          tfd.Normal(loc=t, scale=.5),\n",
    "          reinterpreted_batch_ndims=1),name='prior_lam'),\n",
    "    ])\n",
    "\n",
    "\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    #c = np.log(np.expm1(1.))\n",
    "    return tf.keras.Sequential([\n",
    "      tfp.layers.VariableLayer(2 * n, dtype=dtype,name='post_var'),\n",
    "      tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "          tfd.Normal(loc=t[..., :n],\n",
    "                     scale=tf.nn.softplus(t[..., n:])),\n",
    "          reinterpreted_batch_ndims=1),name='post_lam'),\n",
    "    ])\n",
    "\n",
    "def gen_model(inputs=7,train_len=100):\n",
    "    model = keras.Sequential([keras.layers.Input(inputs,name='Input'),\n",
    "                          keras.layers.Dense(inputs,'sigmoid',name='Dense_Layer'),\n",
    "                          tfpl.DenseVariational(\n",
    "                          units=tfpl.IndependentNormal.params_size(1),\n",
    "                          make_prior_fn=prior_trainable,\n",
    "                          make_posterior_fn=posterior_mean_field,\n",
    "                          kl_weight=1/train_len,\n",
    "                          name='Dense_Variational'),  \n",
    "                          tfpl.IndependentNormal((1,),name='Output_Normal')])\n",
    "    return model\n",
    "\n",
    "\n",
    "def dense_model(inputs=7):\n",
    "    model = keras.Sequential([keras.layers.Input(inputs,name='Input'),  \n",
    "                          keras.layers.Dense(inputs,activation='sigmoid'),\n",
    "                          keras.layers.Dense(6,activation='sigmoid'),  \n",
    "                          keras.layers.Dense(2,activation=None),\n",
    "                          tfpl.IndependentNormal((1,),name='Output_Normal')])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_weights(criteria,df):\n",
    "    new_df = df.copy()\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes=criteria.unique(),\n",
    "                                                 y=criteria)\n",
    "    weight_dict = {}\n",
    "    for idx,i in enumerate(criteria.unique()):\n",
    "        weight_dict[i] = class_weights[idx]\n",
    "    for text in criteria.unique():\n",
    "        new_df.loc[criteria == text,'weight'] = weight_dict[text]\n",
    "        \n",
    "    return new_df.pop('weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e525b256-4c63-4628-bccd-2c09a79e67a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Prob Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034275c-d0b5-481b-8e7f-a5764ad3d203",
   "metadata": {},
   "source": [
    "Cross Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0407e8-1cef-4f11-80e0-39bcf3159782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_train_prob(k):\n",
    "    test_df = pd.read_csv(f'dataframes/crossfold/{k}.csv').set_index('Unnamed: 0')\n",
    "    train_df = all_df.loc[lambda d: ~d.index.isin(test_df.index)]\n",
    "    variables = ['sand','clay','ph','dem','ET','NDVI_500','LST_11','precip','smap']\n",
    "    x = train_df.loc[:,variables]\n",
    "    y = train_df['in_situ']\n",
    "\n",
    "    val_x = test_df.loc[:,variables]\n",
    "    val_y = test_df.in_situ\n",
    "    histories = {}\n",
    "    save_dir = f'Models/crossfold/prob/{k}/'\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                                        monitor='val_loss',\n",
    "                                        factor=0.5,\n",
    "                                        patience=15,\n",
    "                                        verbose=0,\n",
    "                                        mode='auto',\n",
    "                                        min_delta=0.01,#0.0005\n",
    "                                        cooldown=0,\n",
    "                                        min_lr=0.0005)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                        monitor='val_loss',\n",
    "                                        patience=31,\n",
    "                                        min_delta=0.0005)#0.0002\n",
    "\n",
    "    for att in ['texture','sand','clay','koep','mcd12','ph']:\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    '{0}{1}'.format(save_dir,att),\n",
    "                    monitor='val_loss',\n",
    "                    verbose=0,\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    mode='min',\n",
    "                    save_freq='epoch',\n",
    "                )\n",
    "        callbacks = [checkpoint,reduce_lr,early_stopping]\n",
    "        weights = create_weights(train_df[att],train_df)\n",
    "        val_weights = create_weights(test_df[att],test_df)\n",
    "        val_data = (val_x,val_y.values,val_weights)\n",
    "        model = gen_model(inputs=x.shape[-1],train_len=x.shape[0])\n",
    "        model.compile(loss=nll, optimizer=keras.optimizers.Adam(learning_rate=0.1),weighted_metrics=[])\n",
    "        history = model.fit(x=x,y=y,epochs=500,batch_size=8192*2,sample_weight=weights,validation_data=val_data,callbacks=callbacks)\n",
    "        histories[att] = history\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                '{}free'.format(save_dir),\n",
    "                monitor='val_loss',\n",
    "                verbose=0,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                mode='min',\n",
    "                save_freq='epoch',\n",
    "            )\n",
    "\n",
    "    callbacks = [checkpoint,reduce_lr,early_stopping]\n",
    "    val_data = (val_x,val_y.values)\n",
    "    model = gen_model(inputs=x.shape[-1],train_len=x.shape[0])\n",
    "    model.compile(loss=nll, optimizer=keras.optimizers.Adam(learning_rate=0.1))\n",
    "    history = model.fit(x=x,y=y,epochs=500,batch_size=8192*2,validation_data=val_data,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc973fe8-fa45-4f8f-a0fb-0963bd4376db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dense Train\n",
    "\n",
    "def cross_train_dense(k):\n",
    "    test_df = pd.read_csv(f'dataframes/crossfold/{k}.csv').set_index('Unnamed: 0')\n",
    "    train_df = all_df.loc[lambda d: ~d.index.isin(test_df.index)]\n",
    "    variables = ['sand','clay','ph','dem','ET','NDVI_500','LST_11','precip','smap']\n",
    "    x = train_df.loc[:,variables]\n",
    "    y = train_df['in_situ']\n",
    "\n",
    "    val_x = test_df.loc[:,variables]\n",
    "    val_y = test_df.in_situ\n",
    "    histories = {}\n",
    "    save_dir = f'Models/crossfold/dense/{k}/'\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                                        monitor='val_loss',\n",
    "                                        factor=0.5,\n",
    "                                        patience=15,\n",
    "                                        verbose=0,\n",
    "                                        mode='auto',\n",
    "                                        min_delta=0.01,#0.0005\n",
    "                                        cooldown=0,\n",
    "                                        min_lr=0.0005)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                        monitor='val_loss',\n",
    "                                        patience=31,\n",
    "                                        min_delta=0.0005)#0.0002\n",
    "\n",
    "    for att in ['texture','sand','clay','koep','mcd12','ph']:\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    '{0}{1}'.format(save_dir,att),\n",
    "                    monitor='val_loss',\n",
    "                    verbose=0,\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    mode='min',\n",
    "                    save_freq='epoch',\n",
    "                )\n",
    "        callbacks = [checkpoint,reduce_lr,early_stopping]\n",
    "        weights = create_weights(train_df[att],train_df)\n",
    "        val_weights = create_weights(test_df[att],test_df)\n",
    "        val_data = (val_x,val_y.values,val_weights)\n",
    "        model = dense_model(inputs=x.shape[-1])\n",
    "        model.compile(loss=nll, optimizer=keras.optimizers.Adam(learning_rate=0.1),weighted_metrics=[])\n",
    "        history = model.fit(x=x,y=y,epochs=500,batch_size=8192*2,sample_weight=weights,validation_data=val_data,callbacks=callbacks)\n",
    "        histories[att] = history\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                '{}free'.format(save_dir),\n",
    "                monitor='val_loss',\n",
    "                verbose=0,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                mode='min',\n",
    "                save_freq='epoch',\n",
    "            )\n",
    "\n",
    "    callbacks = [checkpoint,reduce_lr,early_stopping]\n",
    "    val_data = (val_x,val_y.values)\n",
    "    model = gen_model(inputs=x.shape[-1],train_len=x.shape[0])\n",
    "    model.compile(loss=nll, optimizer=keras.optimizers.Adam(learning_rate=0.1))\n",
    "    history = model.fit(x=x,y=y,epochs=500,batch_size=8192*2,validation_data=val_data,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b90a14a-e54e-4080-943d-8baceb1621c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    cross_train(k)\n",
    "    cross_train_dense(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f9066-c36b-42fa-ab6c-d99531c1bd4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### WDL Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca88dd-8de7-45bb-a6d6-e70b795ffca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_dict = {'texture':13,'mcd12':18,'koep':32}\n",
    "\n",
    "def wide_inputs(df,spatial=False):\n",
    "    if spatial:\n",
    "        dnn_in = df.loc[:,['sand','clay','ph','dem','NDVI_250','LST_11','ET','precip','smap']]\n",
    "    else:\n",
    "        dnn_in = df.loc[:,['sand','clay','ph','dem','NDVI_500','LST_11','ET','precip','smap']]\n",
    "    wide_in = dnn_in\n",
    "    cats = df.loc[:,['texture','mcd12','koep']]\n",
    "    cat_embeds = []\n",
    "    for cat in cats.columns:\n",
    "        embedding = np.eye(cat_dict[cat])[cats[cat].astype(int)]\n",
    "        cat_embeds.append(np.asarray(embedding))\n",
    "\n",
    "    embeddings = np.concatenate(cat_embeds,axis=1)\n",
    "    dnn_in = np.concatenate([dnn_in,embeddings],axis=1)\n",
    "    return wide_in.values.astype(float),dnn_in.astype(float)\n",
    "\n",
    "from tensorflow.keras.experimental import LinearModel, WideDeepModel\n",
    "def initialize_models():\n",
    "    linear_model = LinearModel()\n",
    "    dnn_model = keras.Sequential([keras.layers.Dense(units=128,activation='sigmoid',name='dnn_1'),\n",
    "                                 keras.layers.Dense(units=64,activation='sigmoid',name='dnn_2'),\n",
    "                                 keras.layers.Dense(units=1,activation='sigmoid',name='dnn_3')])\n",
    "    return linear_model,dnn_model\n",
    "\n",
    "linear_model, dnn_model = initialize_models()\n",
    "\n",
    "model = WideDeepModel(linear_model,dnn_model)\n",
    "model.compile(optimizer=['sgd',keras.optimizers.Adam(learning_rate=0.001)],loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317f2be-7ff1-4e4d-88b8-a52851eaeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WDL Train\n",
    "def cross_train_wdl(k):\n",
    "    test_df = pd.read_csv(f'dataframes/crossfold/{k}.csv').set_index('Unnamed: 0')\n",
    "    train_df = all_df.loc[lambda d: ~d.index.isin(test_df.index)]\n",
    "    variables = ['sand','clay','ph','dem','ET','NDVI_500','LST_11','precip','smap']\n",
    "    wide_x,dnn_x = wide_inputs(train_df)\n",
    "    wide_y,dnn_y = wide_inputs(test_df)\n",
    "    y = train_df.in_situ\n",
    "    val_y = test_df.in_situ\n",
    "    val_data = [[wide_y,dnn_y],val_y]\n",
    "    save_dir = f'Models/crossfold/wdl/{k}/'\n",
    "\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                                        monitor='val_loss',\n",
    "                                        factor=0.5,\n",
    "                                        patience=15,\n",
    "                                        verbose=0,\n",
    "                                        mode='auto',\n",
    "                                        min_delta=0.01,#0.0005\n",
    "                                        cooldown=0,\n",
    "                                        min_lr=0.0005)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                                        monitor='val_loss',\n",
    "                                        patience=31,\n",
    "                                        min_delta=0.0005)#0.0002\n",
    "\n",
    "    for att in ['texture','sand','clay','koep','mcd12','ph']:\n",
    "        linear_model,dnn_model = initialize_models()\n",
    "        \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                    '{0}{1}'.format(save_dir,att),\n",
    "                    monitor='val_loss',\n",
    "                    verbose=0,\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    mode='min',\n",
    "                    save_freq='epoch',\n",
    "                )\n",
    "        callbacks = [checkpoint,reduce_lr,early_stopping]\n",
    "        weights = create_weights(train_df[att],train_df)\n",
    "        val_weights = create_weights(test_df[att],test_df)\n",
    "        val_data =  [[wide_y,dnn_y],val_y,val_weights]\n",
    "        model = WideDeepModel(linear_model,dnn_model)\n",
    "        for lr in [0.001,0.0005,0.0001]:\n",
    "            model.compile(optimizer=['sgd',keras.optimizers.Adam(learning_rate=lr)][1],loss='mse',weighted_metrics=[])\n",
    "            history = model.fit([wide_x,dnn_x],y,epochs=500,batch_size=8192*2,sample_weight=weights,validation_data=val_data,callbacks=callbacks)\n",
    "\n",
    "\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                '{}free'.format(save_dir),\n",
    "                monitor='val_loss',\n",
    "                verbose=0,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                mode='min',\n",
    "                save_freq='epoch',\n",
    "            )\n",
    "\n",
    "    linear_model,dnn_model = initialize_models()\n",
    "    callbacks = [checkpoint,reduce_lr,early_stopping]\n",
    "    val_data = [[wide_y,dnn_y],val_y]\n",
    "    model = WideDeepModel(linear_model,dnn_model)\n",
    "    for lr in [0.001,0.0005,0.0001]:\n",
    "        model.compile(optimizer=['sgd',keras.optimizers.Adam(learning_rate=lr)][1],loss='mse')\n",
    "        history = model.fit([wide_x,dnn_x],y,epochs=500,batch_size=8192*2,validation_data=val_data,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3b31f-0a50-4428-8774-d95f746f899e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    cross_train_wdl(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75649d64-47b8-45f7-ae2d-73a304a6627a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac3347-4b99-44db-bd27-aa660bca8631",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d5a65-96ae-4a24-9387-1624780833fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ubrmse(ground,pred):\n",
    "    bias = np.mean(ground-pred)\n",
    "    rmse = np.sqrt(np.mean((ground-pred)**2))\n",
    "    ubrmse = np.sqrt(rmse**2-bias**2)\n",
    "    return round(ubrmse,4)\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def to_rf(df):\n",
    "    df = df.fillna(-1)\n",
    "    for col in df.columns:\n",
    "        if col in ['dem','smap','precip','in_situ']:\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif col =='date':\n",
    "            continue\n",
    "        else:\n",
    "            df[col] = df[col].astype('int16')\n",
    "    return df\n",
    "\n",
    "def batch_dict(df,spatial):\n",
    "    test_dict = {}\n",
    "    lst,ndvi = spatial\n",
    "    for var in ['sand','clay','ph','dem','ET',ndvi,lst,'smap', 'precip']:\n",
    "        if var == ndvi:\n",
    "            test_dict['NDVI_500'] = df[var].values\n",
    "        elif var == lst:\n",
    "            test_dict['LST_11'] = df[var].values\n",
    "        else:\n",
    "            test_dict[var] = df[var].values\n",
    "    return test_dict\n",
    "\n",
    "def rf_predict(df,model_dict,spatial):\n",
    "    batched_dict = batch_dict(df,spatial)\n",
    "    preds = []\n",
    "    for i in model_dict.values():\n",
    "        preds.append(i.predict_on_batch(batched_dict))\n",
    "    preds = np.asarray(preds)\n",
    "    predictions = np.mean(preds,axis=0)\n",
    "    return predictions.squeeze()\n",
    "\n",
    "def ens_pred(df,model_dict):\n",
    "    preds = []\n",
    "    for i in model_dict.values():\n",
    "        preds.append(i(df.values).mean().numpy().squeeze())\n",
    "    \n",
    "    prediction = np.mean(np.asarray(preds),axis=0).squeeze()\n",
    "    return prediction\n",
    "\n",
    "def wdl_ens(df,model_dict,spatial=False):\n",
    "    wide_in,dnn_in = wide_inputs(df)\n",
    "    preds = []\n",
    "    for i in model_dict.values():\n",
    "        preds.append(i((wide_in,dnn_in)).numpy().squeeze())\n",
    "    return np.mean(np.asarray(preds),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff9b2f-5330-4514-ba13-1bab7f576eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "cross_eval_df = pd.DataFrame()\n",
    "variables = ['sand','clay','ph','dem','ET','NDVI_500','LST_11','precip','smap']\n",
    "\n",
    "for k in tqdm(range(10)):\n",
    "    prob_dict = {}\n",
    "    dense_dict = {}\n",
    "    wdl_dict = {}\n",
    "    for att in ['sand','clay','koep','mcd12','free','ph','texture']:\n",
    "        prob_dict[att] = tf.keras.models.load_model(f'Models/crossfold/prob/{k}/{att}_run',custom_objects={'nll':nll})\n",
    "        dense_dict[att] = tf.keras.models.load_model(f'Models/crossfold/dense/{k}/{att}_run',custom_objects={'nll':nll})\n",
    "        wdl_dict[att] = tf.keras.models.load_model(f'Models/crossfold/wdl/{k}/{att}_run',custom_objects={'nll':nll})\n",
    "    cross_df = pd.read_csv(f'Datasets/Crossfold_Datasets/{k}.csv').set_index('Unnamed: 0')\n",
    "    val_df = cross_df.loc[:,variables]\n",
    "    wdl_pred = wdl_ens(cross_df,wdl_dict)\n",
    "    prob_pred = ens_pred(val_df,prob_dict)\n",
    "    dense_pred = ens_pred(val_df,dense_dict)\n",
    "    val_pred_df = cross_df[['in_situ','smap']]\n",
    "    val_pred_df.loc[:,'d_pred'] = dense_pred\n",
    "    val_pred_df.loc[:,'p_pred'] = prob_pred\n",
    "    val_pred_df.loc[:,'wdl_pred'] = wdl_pred\n",
    "    cross_eval_df = pd.concat([cross_eval_df,val_pred_df])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da0745-5343-4df9-92d5-80a0815bc819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "all_df = pd.read_pickle('Datasets/all_df_final.pkl')\n",
    "rf_eval_df = pd.DataFrame()\n",
    "for k in tqdm(range(10)):\n",
    "    rf_dict = {}\n",
    "    for i in os.listdir(f'Models/crossfold/rf/{k}/'):\n",
    "        if i in rf_dict.keys():\n",
    "            continue\n",
    "        try:\n",
    "            int(i)\n",
    "        except:\n",
    "            continue\n",
    "        rf_dict[i] = tf.keras.models.load_model(f'Models/crossfold/rf/{k}/{i}/')\n",
    "    cross_df = pd.read_csv(f'Datasets/Crossfold_Datasets/{k}.csv').set_index('Unnamed: 0')\n",
    "    cross_df = all_df.loc[cross_df.index.unique()]\n",
    "    cross_rf = to_rf(cross_df.drop('date',axis=1))\n",
    "    rf_pred = rf_predict(cross_rf,rf_dict,['NDVI_500','LST_11'])\n",
    "    val_pred_df = cross_rf[['in_situ','smap']]\n",
    "    val_pred_df['rf_pred'] = rf_pred\n",
    "    rf_eval_df = pd.concat([rf_eval_df,val_pred_df])\n",
    "cross_eval_df.loc[:,'rf_pred'] = rf_eval_df.rf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180112f-ad41-4b35-9f7d-6cd3a2ed9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def station_metrics(df,pred,situ='in_situ'):\n",
    "    let = pred.split('_')[0]\n",
    "    stat_dicts = {}\n",
    "    for stat in df.index.unique():\n",
    "        stat_df = df.loc[stat]\n",
    "        if type(stat_df) == pd.core.series.Series:\n",
    "            continue\n",
    "        stat_r = pearsonr(stat_df[situ],stat_df[pred])[0]\n",
    "        stat_ub = ubrmse(stat_df[situ],stat_df[pred])\n",
    "        stat_b = round(np.mean(stat_df[pred]-stat_df[situ]),3) \n",
    "        stat_dicts[stat] = {'{}_r'.format(let):stat_r,\n",
    "                            '{}_ub'.format(let):stat_ub,\n",
    "                            '{}_b'.format(let):stat_b}\n",
    "    return pd.DataFrame().from_dict(stat_dicts,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18606ff0-cbe9-490b-b9c1-4b53811e7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_mets = station_metrics(cross_eval_df,'p_pred')\n",
    "dense_mets = station_metrics(cross_eval_df,'d_pred')\n",
    "smap_mets = station_metrics(cross_eval_df,'smap')\n",
    "wdl_mets = station_metrics(cross_eval_df,'wdl_pred')\n",
    "rf_mets = station_metrics(cross_eval_df,'rf_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb58f76-7d1b-4f94-afa0-2be3839545eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mets = pd.concat([smap_mets,prob_mets,dense_mets,wdl_mets,rf_mets],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52aa4c-8a62-4acd-869c-db947c763ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "mets.to_pickle('Datasets/cross_eval_mets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bba9541d-c414-4d66-aaf7-7a93e6a38f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMAP</th>\n",
       "      <th>Prob</th>\n",
       "      <th>Dense</th>\n",
       "      <th>Wens</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R</th>\n",
       "      <td>0.562</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ubRMSE</th>\n",
       "      <td>0.065</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bias</th>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SMAP   Prob  Dense   Wens     RF\n",
       "R       0.562  0.621  0.639  0.611  0.572\n",
       "ubRMSE  0.065  0.060  0.058  0.060  0.065\n",
       "Bias    0.023 -0.017  0.000 -0.003  0.004"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = []\n",
    "for var in ['_r','_ub','_b']:\n",
    "    sub = mets[[i for i in mets.columns if var in i]]\n",
    "    avg = sub.mean()\n",
    "    avg.name = var\n",
    "    avg = avg.to_frame().T\n",
    "    avg.columns = ['SMAP','Prob','Dense','Wens','RF']\n",
    "    series.append(avg)\n",
    "avgs = pd.concat(series)\n",
    "avgs.index = ['R','ubRMSE','Bias']\n",
    "avgs.round(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
